# Celery Worker Implementation Plan

## Overview
This document breaks down the QUEUE.md implementation into small, manageable iterations that can be completed and tested incrementally.

## Current State

### ‚úÖ **COMPLETED** (Significantly Advanced)
- ‚úÖ **Database Models**: All models exist with proper fields including `enqueued_at`, `attempt`, `last_run_at`, `started_at`, `finished_at`
- ‚úÖ **PostgreSQL and Redis**: Containers configured and working
- ‚úÖ **API Endpoints**: Complete CRUD operations for posts and schedules
- ‚úÖ **Celery Setup**: Full configuration in `src/celery_app.py` with proper routing and beat schedule
- ‚úÖ **Redis Infrastructure**: Complete implementation in `src/utils/redis_utils.py` with dedupe locks
- ‚úÖ **Publish Task**: Fully implemented in `src/tasks/publish.py` with error handling and retry logic
- ‚úÖ **X API Integration**: Complete Twitter service with token management and dry-run support
- ‚úÖ **Docker Configuration**: Worker and beat services properly configured
- ‚úÖ **Dependencies**: All required packages in `requirements.txt`

### ‚ùå **MISSING** (Critical Components Still Needed)
- ‚ùå **Scheduler Task**: No `src/tasks/scheduler.py` - the core automation engine
- ‚ùå **Schedule Resolution Service**: No `src/services/scheduler_service.py` for computing next run times
- ‚ùå **Metrics Collection**: No `src/tasks/metrics.py` for automated metrics capture
- ‚ùå **State Machine**: No `src/utils/state_machine.py` for atomic state transitions
- ‚ùå **Media Preparation**: No `src/tasks/media.py` for media handling
- ‚ùå **Dead Letter Queue**: No `process_dead_letter` task for failed job handling
- ‚ùå **Observability**: No structured logging, monitoring, or health checks

---

## Implementation Phases

### Phase 1: Foundation Setup (Iteration 1-3)

#### Iteration 1: Install Dependencies & Setup Celery App ‚úÖ **COMPLETED**
**Goal**: Get Celery installed and configured with basic setup

**Tasks:**
1. ‚úÖ Add Celery and Celery Beat to `requirements.txt`
   - ‚úÖ `celery>=5.3.0`
   - ‚úÖ `celery[redis]>=5.3.0`
   - ‚úÖ `flower>=2.0.0` (optional, for monitoring)
2. ‚úÖ Create `src/celery_app.py` with basic configuration
3. ‚úÖ Add Celery to docker-compose worker service
4. ‚úÖ Test that worker can start and connect to Redis

**Deliverables:**
- ‚úÖ Updated `requirements.txt`
- ‚úÖ New file `src/celery_app.py`
- ‚úÖ Updated `docker-compose.yml` worker command

**Test**: ‚úÖ Run `celery -A src.celery_app inspect ping` to verify worker is running

---

#### Iteration 2: Database Schema Updates ‚úÖ **COMPLETED**
**Goal**: Add missing fields to PublishJob and Schedule models

**Tasks:**
1. ‚úÖ Update `PublishJob` model in `src/models.py`:
   - ‚úÖ Add `enqueued_at` (DateTime)
   - ‚úÖ Add `attempt` (Integer, default=0)
   - ‚úÖ Update status enum to include all new states
   - ‚úÖ Add unique constraint on `(schedule_id, planned_at)` via `dedupe_key`
2. ‚úÖ Update `Schedule` model:
   - ‚úÖ Add `last_run_at` (DateTime)
   - ‚úÖ Keep `enabled` field (not renamed to `active`)
3. ‚úÖ Create Alembic migration: `004_update_publish_job_fields.py`
4. ‚úÖ Run migration

**Deliverables:**
- ‚úÖ Updated `src/models.py`
- ‚úÖ New migration file in `migrations/versions/`
- ‚úÖ Migration applied to database

**Test**: ‚úÖ Verify columns exist in database with `\d publish_jobs` in psql

---

#### Iteration 3: Redis Infrastructure Setup ‚úÖ **COMPLETED**
**Goal**: Set up Redis client and helper functions

**Tasks:**
1. ‚úÖ Create `src/utils/redis_utils.py`:
   - ‚úÖ Redis client initialization
   - ‚úÖ `acquire_dedupe_lock()` function
   - ‚úÖ Helper for Redis operations
2. ‚úÖ Update `src/database.py` to include Redis client
3. ‚úÖ Add Redis connection testing

**Deliverables:**
- ‚úÖ New file `src/utils/redis_utils.py`
- ‚úÖ Updated `src/database.py`

**Test**: ‚úÖ Simple script to test Redis connection and lock acquisition

---

### Phase 2: Core Task Implementation (Iterations 4-7)

#### Iteration 4: X API Client ‚úÖ **COMPLETED**
**Goal**: Build a reusable client for X/Twitter API

**Tasks:**
1. ‚úÖ Create `src/services/twitter_service.py`:
   - ‚úÖ Class `XAPIClient` with methods for posting and fetching metrics
   - ‚úÖ Support dry-run mode
   - ‚úÖ Environment-driven configuration
   - ‚úÖ Error handling for rate limits
2. ‚úÖ Add environment variables for X API config
3. ‚úÖ Create unit tests with mocked API calls

**Deliverables:**
- ‚úÖ New file `src/services/twitter_service.py`
- ‚úÖ Updated `.env.example` files
- ‚úÖ Basic unit tests

**Test**: ‚úÖ Test with dry-run mode against mock API

---

#### Iteration 5: Publish Post Task (Basic) ‚úÖ **COMPLETED**
**Goal**: Implement the core publishing task

**Tasks:**
1. ‚úÖ Create `src/tasks/publish.py`:
   - ‚úÖ Task `publish_post(job_id)` with basic structure
   - ‚úÖ Early-exit idempotency check
   - ‚úÖ Basic error handling
   - ‚úÖ Status updates (planned -> running -> succeeded/failed)
2. ‚úÖ Configure task in `celery_app.py` with proper routing
3. ‚úÖ Add to worker in docker-compose

**Deliverables:**
- ‚úÖ New file `src/tasks/__init__.py`
- ‚úÖ New file `src/tasks/publish.py`
- ‚úÖ Updated `src/celery_app.py` with task routing

**Test**: ‚úÖ Create a manual publish job and verify task execution

---

#### Iteration 6: State Machine & Atomic Updates ‚ùå **MISSING**
**Goal**: Implement robust state transitions

**Tasks:**
1. ‚ùå Create `src/utils/state_machine.py`:
   - ‚ùå Class `PublishJobStateMachine` with valid transitions
   - ‚ùå Validation logic
2. ‚ùå Update `publish_post` task to use state machine
3. ‚ùå Implement atomic status updates with `SELECT FOR UPDATE`
4. ‚ùå Add retry logic with exponential backoff

**Deliverables:**
- ‚ùå New file `src/utils/state_machine.py`
- ‚ùå Updated `src/tasks/publish.py`

**Test**: ‚ùå Verify transitions work and invalid transitions are rejected

---

#### Iteration 7: Schedule Resolution Service ‚ùå **MISSING**
**Goal**: Build scheduler logic to compute next run times

**Tasks:**
1. ‚ùå Create `src/services/scheduler_service.py`:
   - ‚ùå Class `ScheduleResolver` with methods for each schedule kind
   - ‚ùå `resolve_one_shot()`, `resolve_cron()`, `resolve_rrule()`
2. ‚ùå Install `rrule` package if needed
3. ‚ùå Add timezone handling

**Deliverables:**
- ‚ùå New file `src/services/scheduler_service.py`
- ‚ùå Updated `requirements.txt` if needed

**Test**: ‚ùå Unit tests for each schedule type

---

### Phase 3: Scheduler & Automation (Iterations 8-10)

#### Iteration 8: Scheduler Task (Beat) ‚ùå **MISSING**
**Goal**: Implement the periodic scheduler tick

**Tasks:**
1. ‚ùå Create `src/tasks/scheduler.py`:
   - ‚ùå Task `scheduler_tick()` that runs every minute
   - ‚ùå Query due schedules with `SELECT FOR UPDATE SKIP LOCKED`
   - ‚ùå Create publish_jobs with proper dedupe locks
   - ‚ùå Update schedule `next_run_at`
2. ‚úÖ Configure Celery Beat schedule in `celery_app.py`
3. ‚úÖ Add beat service to docker-compose

**Deliverables:**
- ‚ùå New file `src/tasks/scheduler.py`
- ‚úÖ Updated `src/celery_app.py` with beat schedule
- ‚úÖ Updated `docker-compose.yml` with beat service

**Test**: ‚ùå Watch scheduler create jobs for active schedules

---

#### Iteration 9: Rate Limiting ‚úÖ **PARTIALLY COMPLETED**
**Goal**: Implement API rate limiting

**Tasks:**
1. ‚úÖ Add rate limit configuration to `celery_app.py`:
   - ‚úÖ Configure `task_annotations` based on API tier
   - ‚úÖ Set rate limits in task decorators
2. ‚úÖ Update publish task with rate limit
3. ‚úÖ Add environment variable `X_API_TIER` (free/basic/pro/enterprise)
4. ‚ùå Monitor rate limit headers from X API

**Deliverables:**
- ‚úÖ Updated `src/celery_app.py`
- ‚úÖ Updated environment files

**Test**: ‚ùå Verify task execution respects rate limits

---

#### Iteration 10: Error Handling & Retry Logic ‚úÖ **PARTIALLY COMPLETED**
**Goal**: Robust error handling with intelligent retries

**Tasks:**
1. ‚ùå Create `src/utils/retry_strategy.py`:
   - ‚ùå Class `RetryStrategy` with transient/permanent error classification
   - ‚ùå Exponential backoff calculator
2. ‚úÖ Update `publish_post` task:
   - ‚úÖ Add retry decorator config
   - ‚úÖ Handle specific error types
   - ‚úÖ Update job status on failure
3. ‚ùå Implement dead letter queue handler stub

**Deliverables:**
- ‚ùå New file `src/utils/retry_strategy.py`
- ‚úÖ Updated `src/tasks/publish.py`

**Test**: ‚ùå Simulate various errors and verify retry behavior

---

### Phase 4: Advanced Features (Iterations 11-13)

#### Iteration 11: Metrics Collection
**Goal**: Automate metrics capture for published posts

**Tasks:**
1. Create `src/tasks/metrics.py`:
   - Task `capture_metrics(x_post_id, stage)`
   - Fetch metrics from X API
   - Store in `metrics_snapshots` table
2. Create `src/utils/metrics_polling.py`:
   - Class `MetricsPollingStrategy` with polling schedule
   - Schedule next capture based on stage
3. Integrate metrics capture into publish flow

**Deliverables:**
- New file `src/tasks/metrics.py`
- New file `src/utils/metrics_polling.py`
- Updated `src/tasks/publish.py`

**Test**: Verify metrics are collected and snapshots stored

---

#### Iteration 12: Media Preparation (Stub)
**Goal**: Prepare framework for media handling

**Tasks:**
1. Create `src/tasks/media.py`:
   - Task `prepare_media(media_refs)` (stub for now)
   - Basic structure for future implementation
2. Add media queue to celery routing
3. Update Post model if needed for media

**Deliverables:**
- New file `src/tasks/media.py`
- Updated routing

**Test**: Task can be called (returns success without doing work)

---

#### Iteration 13: Observability & Monitoring
**Goal**: Add logging and monitoring

**Tasks:**
1. Set up structured logging with structlog in `src/utils/logging_config.py`
2. Create `src/utils/observability.py`:
   - Custom Celery Task class with correlation IDs
   - Bind context vars for logging
3. Add Prometheus metrics export (optional):
   - Counters for job attempts
   - Histograms for job duration
4. Create health check task
5. Add Flower for Celery monitoring

**Deliverables:**
- New file `src/utils/observability.py`
- New file `src/utils/logging_config.py`
- Updated tasks to use structured logging
- Optional Prometheus integration

**Test**: Verify logs include correlation IDs and metrics

---

### Phase 5: Production Readiness (Iterations 14-16)

#### Iteration 14: API Integration & Endpoints
**Goal**: Connect API to new worker system

**Tasks:**
1. Update `src/api/posts.py`:
   - Modify schedule creation to use new system
   - Update instant publish to use Celery task
   - Add endpoint to cancel jobs
2. Add new API endpoints:
   - `POST /api/schedules/{id}/enable`
   - `POST /api/schedules/{id}/disable`
   - `GET /api/jobs/{id}/status`

**Deliverables:**
- Updated `src/api/posts.py`
- Updated `src/main.py` with new routes

**Test**: API can create schedules and trigger jobs

---

#### Iteration 15: Docker & Deployment
**Goal**: Production-ready container setup

**Tasks:**
1. Update `docker-compose.yml`:
   - Add dedicated worker services (publish, metrics, scheduler queues)
   - Add beat service
   - Add flower service for monitoring
   - Configure resource limits
2. Update environment variables documentation
3. Add production docker-compose file
4. Add health checks

**Deliverables:**
- Updated `docker-compose.yml`
- New `docker-compose.prod.yml`
- Updated environment examples

**Test**: Full stack runs and workers process jobs correctly

---

#### Iteration 16: Testing & Validation
**Goal**: Comprehensive testing and documentation

**Tasks:**
1. Write unit tests for:
   - All task functions
   - State machine
   - Scheduler resolution
   - Retry strategy
2. Write integration tests:
   - End-to-end publish flow
   - Schedule resolution
   - Error recovery
3. Load testing:
   - Concurrent jobs
   - Rate limit handling
   - Queue performance
4. Update documentation:
   - README with worker setup
   - Deployment guide
   - Architecture diagram

**Deliverables:**
- Test files in `tests/`
- Updated documentation
- Performance benchmarks

**Test**: All tests pass, system handles production load

---

## üéØ **IMPLEMENTATION STATUS SUMMARY**

### ‚úÖ **COMPLETED ITERATIONS** (5/16)
- **Iteration 1**: Celery Setup ‚úÖ
- **Iteration 2**: Database Schema Updates ‚úÖ  
- **Iteration 3**: Redis Infrastructure ‚úÖ
- **Iteration 4**: X API Client ‚úÖ
- **Iteration 5**: Publish Post Task ‚úÖ

### üîÑ **PARTIALLY COMPLETED** (2/16)
- **Iteration 9**: Rate Limiting (basic setup done, monitoring missing)
- **Iteration 10**: Error Handling (basic retry done, strategy missing)

### ‚ùå **MISSING CRITICAL COMPONENTS** (9/16)
- **Iteration 6**: State Machine & Atomic Updates
- **Iteration 7**: Schedule Resolution Service  
- **Iteration 8**: Scheduler Task (Beat) - **MOST CRITICAL**
- **Iteration 11**: Metrics Collection
- **Iteration 12**: Media Preparation
- **Iteration 13**: Observability & Monitoring
- **Iteration 14**: API Integration & Endpoints
- **Iteration 15**: Docker & Deployment
- **Iteration 16**: Testing & Validation

### üö® **NEXT PRIORITY ITERATIONS**
1. **Iteration 8**: Scheduler Task (Beat) - **CRITICAL** - Without this, no automation
2. **Iteration 7**: Schedule Resolution Service - **CRITICAL** - Needed for scheduler
3. **Iteration 6**: State Machine - **HIGH** - For robust state management
4. **Iteration 11**: Metrics Collection - **MEDIUM** - For analytics

### üìä **COMPLETION STATUS**
- **Foundation**: 100% Complete (5/5 iterations)
- **Core Tasks**: 60% Complete (3/5 iterations) 
- **Scheduler**: 0% Complete (0/3 iterations)
- **Advanced Features**: 0% Complete (0/3 iterations)
- **Production**: 0% Complete (0/3 iterations)

**Overall Progress: 31% Complete (5/16 iterations)**

---

After completing all iterations:

‚úÖ Jobs are created automatically from schedules
‚úÖ Jobs execute reliably with proper retry logic
‚úÖ State transitions are atomic and validated
‚úÖ Rate limiting is enforced
‚úÖ Metrics are collected automatically
‚úÖ System is observable with structured logs
‚úÖ All tests pass
‚úÖ Documentation is complete
‚úÖ System runs stably in production-like environment

---

## Quick Start Checklist

Use this checklist when implementing each iteration:

- [ ] Update code changes
- [ ] Run tests (unit/integration)
- [ ] Update migrations if needed
- [ ] Update docker-compose if needed
- [ ] Update environment variables if needed
- [ ] Test manually (start services, create job, verify execution)
- [ ] Update documentation
- [ ] Commit with descriptive message

---

## Iteration Sizes

Each iteration should be:
- **Small**: Completable in 2-4 hours
- **Testable**: Can be verified independently
- **Incremental**: Builds on previous iterations
- **Reversible**: Can be easily rolled back if needed

---

## Notes

- Start with dry-run mode for all API calls during development
- Use feature flags to control rollout of new system
- Keep existing API working while building new system
- Test each iteration thoroughly before moving to next
- Monitor Redis and database performance as system grows

